{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0dbb3bd",
   "metadata": {},
   "source": [
    "# Pytorch_Geometric : Learning to use PyGeo\n",
    "\n",
    "#### This notebook is adapted from a series of videos from Antonio Longa(links: https://github.com/AntonioLonga/PytorchGeometricTutorial and https://www.youtube.com/watch?v=JtDgmmQ60x8&list=PLGMXrbDNfqTzqxB1IGgimuhtfAhGd8lHF&pp=iAQB )\n",
    "\n",
    "## Part 1 : Introduction and Graph Attention Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33188423",
   "metadata": {},
   "source": [
    "### Import Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf993ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "matplotlib          3.5.2\n",
       "numpy               1.21.5\n",
       "session_info        1.0.0\n",
       "torch               2.0.1+cu117\n",
       "torch_geometric     2.3.1\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "PIL                         9.2.0\n",
       "backcall                    0.2.0\n",
       "cffi                        1.15.1\n",
       "colorama                    0.4.5\n",
       "cycler                      0.10.0\n",
       "cython_runtime              NA\n",
       "dateutil                    2.8.2\n",
       "debugpy                     1.5.1\n",
       "decorator                   5.1.1\n",
       "defusedxml                  0.7.1\n",
       "dill                        0.3.4\n",
       "entrypoints                 0.4\n",
       "ipykernel                   6.15.2\n",
       "ipython_genutils            0.2.0\n",
       "jedi                        0.18.1\n",
       "jupyter_server              1.18.1\n",
       "kiwisolver                  1.4.2\n",
       "mkl                         2.4.0\n",
       "mpl_toolkits                NA\n",
       "mpmath                      1.2.1\n",
       "nt                          NA\n",
       "ntsecuritycon               NA\n",
       "nvfuser                     NA\n",
       "packaging                   21.3\n",
       "parso                       0.8.3\n",
       "pickleshare                 0.7.5\n",
       "pkg_resources               NA\n",
       "prompt_toolkit              3.0.20\n",
       "psutil                      5.9.0\n",
       "pydev_ipython               NA\n",
       "pydevconsole                NA\n",
       "pydevd                      2.6.0\n",
       "pydevd_concurrency_analyser NA\n",
       "pydevd_file_utils           NA\n",
       "pydevd_plugins              NA\n",
       "pydevd_tracing              NA\n",
       "pygments                    2.11.2\n",
       "pyparsing                   3.0.9\n",
       "pythoncom                   NA\n",
       "pywintypes                  NA\n",
       "ruamel                      NA\n",
       "scipy                       1.9.1\n",
       "six                         1.16.0\n",
       "sphinxcontrib               NA\n",
       "storemagic                  NA\n",
       "sympy                       1.10.1\n",
       "torch_cluster               1.6.1+pt20cu117\n",
       "torch_scatter               2.1.1+pt20cu117\n",
       "torch_sparse                0.6.17+pt20cu117\n",
       "torch_spline_conv           1.2.2+pt20cu117\n",
       "tornado                     6.1\n",
       "tqdm                        4.64.1\n",
       "traitlets                   5.1.1\n",
       "typing_extensions           NA\n",
       "wcwidth                     0.2.5\n",
       "win32api                    NA\n",
       "win32com                    NA\n",
       "win32security               NA\n",
       "zmq                         23.2.0\n",
       "zope                        NA\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             7.31.1\n",
       "jupyter_client      7.3.4\n",
       "jupyter_core        4.11.1\n",
       "jupyterlab          3.4.4\n",
       "notebook            6.4.12\n",
       "-----\n",
       "Python 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]\n",
       "Windows-10-10.0.22621-SP0\n",
       "-----\n",
       "Session information updated at 2023-05-19 10:15\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for Dataset exploration\n",
    "import torch_geometric\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# for Basic GNN\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "#for GAT\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import session_info\n",
    "\n",
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6530c675",
   "metadata": {},
   "source": [
    "### Dataset exploration from PyGeo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d7f3e5",
   "metadata": {},
   "source": [
    "#### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73e8b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= Planetoid(root='exploration_cora',name=\"Cora\")# Note that pytorch will create a file containing the download and will not re-download it if we recall this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ade42",
   "metadata": {},
   "source": [
    "#### Dataset properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a11f0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cora()\n",
      "Number of graphs: 1\n",
      "Number of classes: 7\n",
      "Number of node features: 1433\n",
      "Number of edge features: 0\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "print(f'Number of node features: {dataset.num_node_features}')\n",
    "print(f'Number of edge features: {dataset.num_edge_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7f9c98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "print(dataset._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a60be709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mask: torch.Size([2708, 1433])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(f'train_mask: {dataset._data.x.shape}')\n",
    "print(dataset._data.x) # x is the list of features for each nodes of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a8d7a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index: torch.Size([2, 10556])\n",
      "tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
      "        [ 633, 1862, 2582,  ...,  598, 1473, 2706]])\n"
     ]
    }
   ],
   "source": [
    "print(f'edge_index: {dataset._data.edge_index.shape}')# we have two list containning 10556 nodes\n",
    "print(dataset._data.edge_index)# the first list in the results give us the start of the edge, the second give us its edge partner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b7fbbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mask: torch.Size([2708])\n",
      "tensor([3, 4, 4,  ..., 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(f'train_mask: {dataset._data.y.shape}')\n",
    "print(dataset._data.y) # y is the category of each nodes of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "910eda00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mask: torch.Size([2708])\n",
      "tensor([ True,  True,  True,  ..., False, False, False])\n"
     ]
    }
   ],
   "source": [
    "print(f'train_mask: {dataset._data.train_mask.shape}')\n",
    "print(dataset._data.train_mask) # list of boleean that give us nodes to consider for the training part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7da01b",
   "metadata": {},
   "source": [
    "### Basic GNN with PyGeo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be0d60",
   "metadata": {},
   "source": [
    "#### Taking the only dataset to feed our GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14427cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d68dfc",
   "metadata": {},
   "source": [
    "#### Define the model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d46d24bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.gnn1= SAGEConv(dataset.num_features,\n",
    "                           dataset.num_classes, \n",
    "                            aggr= 'max' # could be min,mean,add or a personnalized aggregation function\n",
    "                           )\n",
    "    def forward(self):\n",
    "        x= self.gnn1(data.x,data.edge_index)\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dc0788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Net().to(device)\n",
    "data= data.to(device)\n",
    "\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr= 0.001, weight_decay=0.0005)\n",
    "num_epoch= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c547d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10/100, Val = 0.534, Test = 0.545\n",
      "epoch 20/100, Val = 0.654, Test = 0.685\n",
      "epoch 30/100, Val = 0.694, Test = 0.711\n",
      "epoch 40/100, Val = 0.696, Test = 0.712\n",
      "epoch 50/100, Val = 0.696, Test = 0.712\n",
      "epoch 60/100, Val = 0.704, Test = 0.712\n",
      "epoch 70/100, Val = 0.712, Test = 0.716\n",
      "epoch 80/100, Val = 0.712, Test = 0.716\n",
      "epoch 90/100, Val = 0.714, Test = 0.710\n",
      "epoch 100/100, Val = 0.714, Test = 0.710\n"
     ]
    }
   ],
   "source": [
    "#Training loop:\n",
    "best_val_acc=0\n",
    "test_acc=0\n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss = F.nll_loss(model()[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "        \n",
    "    # Validation and test set\n",
    "    with torch.no_grad():\n",
    "        logits,accs = model(), []\n",
    "        for _, mask in data('train_mask','val_mask', 'test_mask'):\n",
    "            pred = logits[mask].max(1)[1]\n",
    "            acc= pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "            accs.append(acc)\n",
    "        _,val_acc,tmp_test_acc= accs\n",
    "        if val_acc> best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            test_acc=tmp_test_acc\n",
    "            \n",
    "            \n",
    "    if (epoch+1) % 10 ==0:\n",
    "        print (f'epoch {epoch+1}/{num_epoch}, Val = {best_val_acc:.3f}, Test = {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f7738b",
   "metadata": {},
   "source": [
    "### Graph Attention Network (GAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751953bb",
   "metadata": {},
   "source": [
    "#### How build a GAT layer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257379da",
   "metadata": {},
   "source": [
    "##### first, build the layers we want to implement with an attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "814dde97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Define parameters of the layer\n",
    "\n",
    "in_features=5\n",
    "out_feature= 2\n",
    "nb_nodes=3\n",
    "\n",
    "#initialize the weight matrix for a linear transformation\n",
    "\n",
    "W= nn.Parameter(torch.zeros(size=(in_features,out_feature)))\n",
    "nn.init.xavier_uniform_(W.data, gain=1.414)\n",
    "\n",
    "#genarate the input\n",
    "inputs= torch.rand(nb_nodes,in_features)\n",
    "\n",
    "#apply the linear transformation\n",
    "h= torch.mm(inputs,W)\n",
    "N=h.size()[0] #number of nodes in outputs\n",
    "print(h.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b4f0e7",
   "metadata": {},
   "source": [
    "##### second, initialize the attention mechanism "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5bfda1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "a= nn.Parameter(torch.zeros(size=(2*out_feature,1)))# the attention mechanism needs to take 2 times the number of outputs of the layer\n",
    "nn.init.xavier_uniform_(a.data,gain=1.414)\n",
    "print(a.shape)\n",
    "\n",
    "leakRelu=nn.LeakyReLU(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a66fa35",
   "metadata": {},
   "source": [
    "##### operation that give all possible node interaction included the self interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "523d2718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "a_input=  torch.cat([h.repeat(1,N).view(N * N,-1),h.repeat(N,1)],dim=1).view(N,-1,2*out_feature)\n",
    "print(a_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aca1a2",
   "metadata": {},
   "source": [
    "##### combine the output of the layer and of the attention mechanism in the activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "456d1bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 1])\n",
      "torch.Size([3, 3])\n",
      "tensor([[-0.2809, -0.1466, -0.3159],\n",
      "        [-0.2783, -0.1441, -0.3133],\n",
      "        [-0.3298, -0.1956, -0.3648]], grad_fn=<LeakyReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "e= leakRelu(torch.matmul(a_input,a).squeeze(2))\n",
    "print(torch.matmul(a_input,a).shape)\n",
    "print(e.shape)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00ebb8",
   "metadata": {},
   "source": [
    "##### for the moment, the attention is calculated on the whole nodes of the graph without taking into account the adjacency of the nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f524f1",
   "metadata": {},
   "source": [
    "##### lets add a mask to include only the adjacent nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9767d63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 0],\n",
      "        [1, 0, 1]])\n",
      "tensor([[-2.8088e-01, -1.4665e-01, -3.1587e-01],\n",
      "        [-2.7831e-01, -1.4408e-01, -9.0000e+15],\n",
      "        [-3.2985e-01, -9.0000e+15, -3.6484e-01]], grad_fn=<WhereBackward0>)\n",
      "tensor([[0.3216, 0.3678, 0.3106],\n",
      "        [0.4665, 0.5335, 0.0000],\n",
      "        [0.5087, 0.0000, 0.4913]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# define the adjacency matrix of our graph\n",
    "adj=torch.randint(2, size=(nb_nodes,nb_nodes))\n",
    "print(adj)\n",
    "\n",
    "zero_vec = -9e15*torch.ones_like(e) # generate a matrix of size of e with -inf like values\n",
    "\n",
    "# define where attention is needed\n",
    "attention= torch.where(adj>0, e, zero_vec)# when two nodes are connected, attention keep the values as calculated previously but if there are no connection, the value is set to the -inf like value\n",
    "print(attention)\n",
    "# apply the softmax activation to the attention mechanism\n",
    "attention=F.softmax(attention,dim=1)\n",
    "print(attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b0a37",
   "metadata": {},
   "source": [
    "#####  apply the masked attention to the previous calculated outputs of the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1cdfc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9707, -0.8190],\n",
      "        [-0.8175, -0.6210],\n",
      "        [-1.1527, -1.1179]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.9999, -0.9819],\n",
      "        [-0.6580, -0.3054],\n",
      "        [-1.3109, -1.2588]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h_prime= torch.matmul(attention,h)\n",
    "print(h_prime)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9befca80",
   "metadata": {},
   "source": [
    "#####  define the GATLayer from the previous work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b7527e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT_layer(nn.Module):\n",
    "    def __init__(self,in_features, out_features, dropout,alpha,concat=True):\n",
    "        super (GAT_layer,self).__init__()\n",
    "        self.dropout= dropout\n",
    "        self.in_features=in_features\n",
    "        self.out_features= out_features\n",
    "        self.alpha=alpha # to define how LeakyRelu handle negative values\n",
    "        self.concat=concat\n",
    "\n",
    "        self.W= nn.Parameter(torch.zeros(size=(self.in_features,self.out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        \n",
    "        self.a= nn.Parameter(torch.zeros(size=(2*self.out_features,1)))\n",
    "        nn.init.xavier_uniform_(self.a.data,gain=1.414)\n",
    "        \n",
    "        self.leakyrelu =nn.LeakyReLU(self.alpha)\n",
    "        \n",
    "    def forward(self,inputs,adj):\n",
    "        \n",
    "        #linear transformation\n",
    "        h= torch.mm(inputs,self.W)\n",
    "        N=h.size()[0]\n",
    "        \n",
    "        #attention\n",
    "        a_input=  torch.cat([h.repeat(1,N).view(N * N,-1),h.repeat(N,1)],dim=1).view(N,-1,2*self.out_features)\n",
    "        e= self.leakyrelu(torch.matmul(a_input,self.a).squeeze(2))\n",
    "        \n",
    "        #masked attention\n",
    "        zero_vec = -9e15*torch.ones_like(e) \n",
    "        attention= torch.where(adj>0, e, zero_vec)\n",
    "        attention=F.softmax(attention,dim=1)\n",
    "        attention= F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime= torch.matmul(attention,h)\n",
    "        \n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe8972",
   "metadata": {},
   "source": [
    "##### This is just an implementation to understand how GAT works, the pre-built PyGeo Class is faster and will be use next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca301cf",
   "metadata": {},
   "source": [
    "#### Use the GAT_layer on an exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd9e2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already have import the Cora dataset, we just apply a transformation to normalize the features\n",
    "dataset= Planetoid(root='exploration_cora',name=\"Cora\", transform= T.NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f750bef",
   "metadata": {},
   "source": [
    "##### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4529e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT,self).__init__()\n",
    "        self.hidden= 8\n",
    "        self.in_head= 8\n",
    "        self.out_head= 1\n",
    "        \n",
    "        \n",
    "        self.conv1= GATConv(dataset.num_features, self.hidden, heads= self.in_head,dropout = 0.6)\n",
    "        self.conv2= GATConv(self.hidden*self.in_head, dataset.num_classes, concat= False,heads= self.out_head, dropout=0.6)\n",
    "        \n",
    "    def forward(self,data):\n",
    "        \n",
    "        x, edge_index = data.x,data.edge_index\n",
    "        \n",
    "        x= F.dropout(x,p= 0.6, training= self.training)\n",
    "        x= self.conv1(x,edge_index)\n",
    "        x= F.elu(x)\n",
    "        x= F.dropout(x,p= 0.6, training= self.training)\n",
    "        x= self.conv2(x,edge_index)\n",
    "        \n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee84022",
   "metadata": {},
   "source": [
    "##### Define optimizer and push everything on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cb11e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= GAT().to(device)\n",
    "data= dataset[0].to(device)\n",
    "\n",
    "optimizer=  torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay= 8e-4)\n",
    "num_epoch=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0269a046",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "787cf693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100/1000, Loss= 0.994\n",
      "epoch 200/1000, Loss= 0.775\n",
      "epoch 300/1000, Loss= 0.739\n",
      "epoch 400/1000, Loss= 0.623\n",
      "epoch 500/1000, Loss= 0.693\n",
      "epoch 600/1000, Loss= 0.698\n",
      "epoch 700/1000, Loss= 0.581\n",
      "epoch 800/1000, Loss= 0.703\n",
      "epoch 900/1000, Loss= 0.578\n",
      "epoch 1000/1000, Loss= 0.669\n"
     ]
    }
   ],
   "source": [
    "#Training loop:\n",
    "\n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs= model(data)\n",
    "    loss = F.nll_loss(outputs[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 ==0:\n",
    "        print (f'epoch {epoch+1}/{num_epoch}, Loss= {loss:.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60a1c1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.835\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "_,pred= model(data).max(dim=1)\n",
    "correct= float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc= correct / data.test_mask.sum().item()\n",
    "print(f'Accuracy = {acc:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
